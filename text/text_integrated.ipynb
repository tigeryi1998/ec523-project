{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7940b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer,AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb6332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    \n",
    "    def __init__(self, model: nn.Module, decay_rate, device=None):\n",
    "        # copy the model, this model is the model that is in \"target\" mode\n",
    "        self.model = copy.deepcopy(model)\n",
    "        # model should not be changed by calls to backward()\n",
    "        self.model.requires_grad_(False)\n",
    "        # update model if training on CUDA\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        # set decay rate\n",
    "        self.decay = decay_rate\n",
    "        # the number of times we have stepped the EMA, needed to eventually taper down delta\n",
    "        self.update_count = 0\n",
    "        \n",
    "    def step(self, n_model: nn.Module):\n",
    "        \"\"\"\n",
    "        A single step of the EMA parameterization of the teacher's parameters\n",
    "\n",
    "        Args:\n",
    "            n_model (nn.Module): this is the student model, that the teacher will be following with EMA\n",
    "        \"\"\"\n",
    "        # current parameters\n",
    "        ema_state_dict = {}\n",
    "        ema_params = self.model.state_dict()\n",
    "        \n",
    "        # update parameters as a the exponential moving average of n_model\n",
    "        for key, param in n_model.state_dict().items():\n",
    "            xx = ema_params[key].float()\n",
    "            xx.mul_(self.decay_rate)\n",
    "            xx = xx.add(param.to(dtype=xx.dtype).mul(1-self.decay))\n",
    "            ema_state_dict[key] = xx\n",
    "        \n",
    "        # load the updated parameters back into model\n",
    "        self.model.load_state_dict(ema_state_dict, strict=False)\n",
    "        # update count\n",
    "        self.update_count += 1\n",
    "        \n",
    "        \n",
    "    def set_decay(self, decay_rate):\n",
    "        self.decay_rate = decay_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f76e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, modality, embed_dim, ema_decay, ema_end_decay, ema_anneal_end_step, device):\n",
    "        self.encoder = encoder\n",
    "        self.modality = modality\n",
    "        self.embed_dim = embed_dim\n",
    "        self.EMAModule = EMA(self.encoder, decay_rate=ema_decay, device=device)\n",
    "        self.device = device\n",
    "        \n",
    "        # building the regression head\n",
    "        if self.modality == 'text':\n",
    "            self.regression_head = None\n",
    "        elif self.modality == 'image':\n",
    "            self.regression_head = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        else:\n",
    "            raise Exception('Given modality is not accepted: ', str(modality))\n",
    "            \n",
    "        # parameters for the EMA module\n",
    "        self.ema_decay = ema_decay                          # starting decay rate\n",
    "        self.ema_end_decay = ema_end_decay                  # ending decay rate\n",
    "        self.ema_anneal_end_step = ema_anneal_end_step      # how many steps it should take to reach decay rate (at max)\n",
    "        \n",
    "        \n",
    "    def stepEMA(self):\n",
    "        '''\n",
    "        Performs a step in the EMA module (as the teacher model)\n",
    "        '''\n",
    "        # find new decay rate if necessary\n",
    "        if self.ema_decay >= self.ema_end_decay:\n",
    "            if self.EMAModule.update_count >= self.ema_anneal_end_step:\n",
    "                # get decay as the ending decay\n",
    "                decay = self.ema_end_decay\n",
    "            else:\n",
    "                # get decay based on update count\n",
    "                delta = self.ema_end_decay - self.ema_decay\n",
    "                updates_remaining = 1 - (self.EMAModule.update_count / self.ema_anneal_end_step)\n",
    "                decay = self.ema_end_decay - (delta * updates_remaining) \n",
    "            \n",
    "            # if we changed, set new decay rate in EMA module\n",
    "            self.EMAModule.set_decay(decay)\n",
    "            \n",
    "        # perform step in EMA\n",
    "        if self.EMAModule.decay < 1:\n",
    "            self.EMAModule.step()\n",
    "        \n",
    "    def forward(self, src, k, target=None, mask=None):\n",
    "        x = self.encoder(src, mask)['output']                                       # 1) pass source thru encoder (with mask), and get encoded rep\n",
    "        \n",
    "        # if we are in Student mode, we do not have a target, and we are simply building an encoded representation\n",
    "        if target==None:\n",
    "            return x\n",
    "        \n",
    "        # if we are in Teacher mode, we need to evualate syste\n",
    "        with torch.no_grad():\n",
    "            self.EMAModule.model.eval()\n",
    "            y = self.EMAModule.model(target, ~mask)['states']                       # 2) Get transformer layers outputs \n",
    "            y = y[:k]                                                               # 3) Only keep top k layers outputs\n",
    "            \n",
    "            # normalizing layers\n",
    "            y = [F.layer_norm(layer.float(), layer.shape[-1:]) for layer in y]      # 4) Normalize all outputs\n",
    "            y = sum(y) / len(y)                                                     # 5) Get avg output across all top k layers\n",
    "            \n",
    "        x = self.regression_head(x[mask])                                           # 6) Regress x with mask (linear layer)\n",
    "        y = y[mask]                                                                 # 7) Apply mask to y\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3076e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiText(Dataset):\n",
    "    \"\"\"\n",
    "    A Dataset instance for WikiText dataset loaded from HuggingFace datasets.\n",
    "\n",
    "    Args:\n",
    "        cfg (DictConfig): config object\n",
    "        split: Split to load ['train', 'test']\n",
    "        tokenizer: A HuggingFace Tokenizer model like BPE\n",
    "        **kwargs: extra args which are set as dataset properties\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, split, tokenizer, **kwargs):\n",
    "        super(WikiText, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.path = cfg.dataset.name\n",
    "        self.mlm_probability = cfg.dataset.mlm_probability\n",
    "        raw_data = load_dataset('wikitext', self.path)[split]\n",
    "        self.data = self.clean_dataset(raw_data) if self.cfg.dataset.clean_dataset else raw_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def clean_dataset(self, data):\n",
    "        \"\"\"\n",
    "        Cleanup dataset by removing invalid sized samples, etc.\n",
    "        \"\"\"\n",
    "        print('Cleaning dataset ...')\n",
    "        min_seq_len, max_seq_len = self.cfg.dataset.valid_seq_lenghts\n",
    "        texts = []\n",
    "        with tqdm(data, desc='Removing invalid sized inputs: ') as tbar:\n",
    "            for i, x in enumerate(tbar):\n",
    "                if len(x['text']) in range(min_seq_len, max_seq_len + 1):\n",
    "                    texts.append(x)\n",
    "        return texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Only return tokens from raw text with no additions e.g, padding, bos/eos, etc.\n",
    "        Args:\n",
    "            index: sample index to pick from dataset\n",
    "\n",
    "        Returns:\n",
    "            tokenized outputs\n",
    "        \"\"\"\n",
    "        raw_text = self.data[index]['text']\n",
    "        tokens = self.tokenizer(raw_text, return_attention_mask=False)\n",
    "        return tokens\n",
    "\n",
    "    def _mask_tokens(self, inputs, special_tokens_mask=None):\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Ported\n",
    "         from `transformers.data.DataCollatorForLanguageModeling.torch_mask_tokens()`\n",
    "        Args:\n",
    "            inputs: batch of input tokens\n",
    "            special_tokens_mask:\n",
    "\n",
    "        Returns:\n",
    "            a dict batch of masked and padded inputs/labels\n",
    "\n",
    "        \"\"\"\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in\n",
    "                labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = self.tokenizer.pad_token_id\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels, masked_indices\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Collate the batch of data using BERT masking strategy. carefully ported from\n",
    "         transformers.data.DataCollatorForLanguageModeling\n",
    "        Args:\n",
    "            batch: batch of data\n",
    "\n",
    "        Returns:\n",
    "            same batch of data masked and padded\n",
    "        \"\"\"\n",
    "        batch = self.tokenizer.pad(batch, return_tensors=\"pt\")\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        src, trg, masked_indices = self._mask_tokens(\n",
    "            batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "        )\n",
    "        return src, trg, masked_indices\n",
    "\n",
    "\n",
    "#         batch = self.tokenizer.pad(batch, return_tensors=\"pt\")\n",
    "#         # If special token mask has been preprocessed, pop it from the dict.\n",
    "#         special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "#         src, trg, masked_indices = self._mask_tokens(\n",
    "#             batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "#         )\n",
    "#         return {\n",
    "#         'input_ids': src,\n",
    "#         'labels': trg,\n",
    "#         'masked_indices': masked_indices\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89aaf246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder model using HuggingFace for NLP\n",
    "\n",
    "    To load your desired model specify model checkpoint under cfg.model.encoder_checkpoint\n",
    "\n",
    "    Args:\n",
    "        cfg: An omegaconf.DictConf instance containing all the configurations.\n",
    "        **kwargs: extra args which are set as model properties\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, **kwargs):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        checkpoint = cfg.model.encoder_checkpoint\n",
    "        model_config = AutoConfig.from_pretrained(checkpoint)\n",
    "        self.encoder = AutoModel.from_config(model_config)\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def forward(self, inputs, mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward inputs through the encoder and extract transformer/attention layers outputs\n",
    "\n",
    "        Args:\n",
    "            inputs: source tokens\n",
    "            mask: bool masked indices\n",
    "            kwargs: keyword args specific to the encoder's forward method\n",
    "\n",
    "        Returns:\n",
    "            A dictionary of the encoder outputs including transformer layers outputs and attentions outputs\n",
    "\n",
    "        \"\"\"\n",
    "        # Note: inputs are already masked for MLM so mask is not used\n",
    "        outputs = self.encoder(inputs, output_hidden_states=True, output_attentions=True, **kwargs)\n",
    "        encoder_states = outputs['hidden_states'][:-1]  # encoder layers outputs separately\n",
    "        encoder_out = outputs['hidden_states'][-1]      # last encoder output (accumulated)\n",
    "        attentions = outputs['attentions']\n",
    "        return {\n",
    "            'encoder_states': encoder_states,\n",
    "            'encoder_out': encoder_out,\n",
    "            'attentions': attentions\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0344f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def maybe_save_checkpoint(model, optimizer, path, epoch_num, save_freq):\n",
    "    \"\"\"\n",
    "    Save a checkpoint specific to Data2Vec\n",
    "    Args:\n",
    "        model: a nn.Module instance\n",
    "        optimizer\n",
    "        path: path to save checkpoint to\n",
    "        epoch_num: current epoch number\n",
    "        save_freq: save frequency based on epoch number\n",
    "\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    path = os.path.join(path, f'{epoch_num}.pt')\n",
    "    if epoch_num % save_freq == 0:\n",
    "        checkpoint = {'data2vec': model.state_dict(),\n",
    "                      'encoder': model.encoder.encoder.state_dict(),\n",
    "                      'optimizer': optimizer.state_dict()}\n",
    "        torch.save(checkpoint, path)\n",
    "        print(f'Saved checkpoint to `{path}`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9f1bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTrainer:\n",
    "    \"\"\"\n",
    "    A Trainer class to train and evaluate NLP models using the Data2Vec approach.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The Data2Vec model to be trained.\n",
    "        tokenizer: Tokenizer for preparing input data.\n",
    "        train_dataset (Dataset): Dataset for training.\n",
    "        test_dataset (Dataset): Dataset for evaluation.\n",
    "        config (DictConfig): Configuration object containing training parameters and device information.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        self.config = cfg\n",
    "        self.model = Data2Vec(encoder=self.encoder, cfg=cfg)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(cfg.model.encoder_checkpoint)\n",
    "        self.train_dataset = WikiText(cfg, 'train', self.tokenizer)\n",
    "        self.test_dataset = WikiText(cfg, 'test', self.tokenizer)\n",
    "        self.config = cfg\n",
    "\n",
    "        \n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), cfg.optimizer.lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()  # Adjust according to your loss function needs\n",
    "\n",
    "    def run_epoch(self, mode='train'):\n",
    "        \"\"\"\n",
    "        Run one epoch of training or evaluation.\n",
    "\n",
    "        Args:\n",
    "            mode (str): Specifies the mode 'train' or 'eval'.\n",
    "\n",
    "        Returns:\n",
    "            Average loss of the epoch.\n",
    "        \"\"\"\n",
    "        if mode == 'train':\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(self.train_loader if mode == 'train' else self.test_loader, desc=f\"{mode.capitalize()} Epoch\"):\n",
    "            inputs, labels = batch['input_ids'].to(self.config.device), batch['labels'].to(self.config.device)\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "            if mode == 'train':\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(self.train_loader.dataset if mode == 'train' else self.test_loader.dataset)\n",
    "        print(f\"{mode.capitalize()} loss: {avg_loss:.4f}\")\n",
    "        return avg_loss\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        \"\"\"\n",
    "        Train the model for a given number of epochs, alternating between training and evaluation.\n",
    "\n",
    "        Args:\n",
    "            num_epochs (int): Number of epochs to train the model.\n",
    "        \"\"\"\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            self.run_epoch('train')\n",
    "            with torch.no_grad():\n",
    "                self.run_epoch('eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ee63e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "cfg = OmegaConf.load('roberta-pretraining.yaml')  # Adjust the path if necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547db90",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TextTrainer(cfg)\n",
    "num_epochs = cfg.train.num_epochs  # Number of epochs to train\n",
    "trainer.train(num_epochs)\n",
    "evaluation_loss = trainer.run_epoch(mode='eval')\n",
    "print(f\"Final Evaluation Loss: {evaluation_loss}\")\n",
    "trained_model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample text data\n",
    "examples = [\n",
    "    \"A totally engrossing thriller.\",\n",
    "    \"Unfortunately, the story is not as strong as the direction or the atmosphere.\"\n",
    "]\n",
    "\n",
    "# Load tokenizer and model from a checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model.encoder_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(trained_model)\n",
    "model.eval()\n",
    "\n",
    "# Preprocess the examples\n",
    "inputs = tokenizer(examples, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Run the model on the examples\n",
    "with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Process the results\n",
    "predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "# Map predictions to labels (for SST-2: 0 = negative, 1 = positive)\n",
    "labels = [\"negative\", \"positive\"]\n",
    "predicted_labels = [labels[p] for p in predictions]\n",
    "\n",
    "# Show results\n",
    "for text, label in zip(examples, predicted_labels):\n",
    "    print(f\"Text: {text}\\nPredicted sentiment: {label}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
