{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7940b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import copy \n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb6332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    \"\"\"\n",
    "    Modified version of class fairseq.models.ema.EMAModule.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module):\n",
    "        cfg (DictConfig):\n",
    "        device (str):\n",
    "        skip_keys (list): The keys to skip assigning averaged weights to.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module, cfg, skip_keys=None):\n",
    "        self.model = self.deepcopy_model(model)\n",
    "        self.model.requires_grad_(False)\n",
    "        self.cfg = cfg\n",
    "        self.device = cfg.device\n",
    "        self.model.to(self.device)\n",
    "        self.skip_keys = skip_keys or set()\n",
    "        self.decay = self.cfg.model.ema_decay\n",
    "        self.num_updates = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def deepcopy_model(model):\n",
    "        try:\n",
    "            model = copy.deepcopy(model)\n",
    "            return model\n",
    "        except RuntimeError:\n",
    "            tmp_path = 'tmp_model_for_ema_deepcopy.pt'\n",
    "            torch.save(model, tmp_path)\n",
    "            model = torch.load(tmp_path)\n",
    "            os.remove(tmp_path)\n",
    "            return model\n",
    "\n",
    "    def step(self, new_model: nn.Module):\n",
    "        \"\"\"\n",
    "        One EMA step\n",
    "\n",
    "        Args:\n",
    "            new_model (nn.Module): Online model to fetch new weights from\n",
    "\n",
    "        \"\"\"\n",
    "        ema_state_dict = {}\n",
    "        ema_params = self.model.state_dict()\n",
    "        for key, param in new_model.state_dict().items():\n",
    "            ema_param = ema_params[key].float()\n",
    "            if key in self.skip_keys:\n",
    "                ema_param = param.to(dtype=ema_param.dtype).clone()\n",
    "            else:\n",
    "                ema_param.mul_(self.decay)\n",
    "                ema_param.add_(param.to(dtype=ema_param.dtype), alpha=1 - self.decay)\n",
    "            ema_state_dict[key] = ema_param\n",
    "        self.model.load_state_dict(ema_state_dict, strict=False)\n",
    "        self.num_updates += 1\n",
    "\n",
    "    def restore(self, model: nn.Module):\n",
    "        \"\"\"\n",
    "        Reassign weights from another model\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): model to load weights from.\n",
    "\n",
    "        Returns:\n",
    "            model with new weights\n",
    "        \"\"\"\n",
    "        d = self.model.state_dict()\n",
    "        model.load_state_dict(d, strict=False)\n",
    "        return model\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_annealed_rate(start, end, curr_step, total_steps):\n",
    "        \"\"\"\n",
    "        Calculate EMA annealing rate\n",
    "        \"\"\"\n",
    "        r = end - start\n",
    "        pct_remaining = 1 - curr_step / total_steps\n",
    "        return end - r * pct_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d75391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data2Vec(nn.Module):\n",
    "    \"\"\"\n",
    "    Data2Vec main module.\n",
    "\n",
    "    Args:\n",
    "         encoder (nn.Module): The encoder module like BEiT, ViT, etc.\n",
    "         cfg (omegaconf.DictConfig): The config containing model properties\n",
    "    \"\"\"\n",
    "    MODALITIES = ['vision', 'text', 'audio']\n",
    "\n",
    "    def __init__(self, encoder, cfg, **kwargs):\n",
    "        super(Data2Vec, self).__init__()\n",
    "        self.modality = cfg.modality\n",
    "        self.embed_dim = cfg.model.embed_dim\n",
    "        self.encoder = encoder\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.ema = EMA(self.encoder, cfg)  # EMA acts as the teacher\n",
    "        self.regression_head = self._build_regression_head()\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.ema_decay = self.cfg.model.ema_decay\n",
    "        self.ema_end_decay = self.cfg.model.ema_end_decay\n",
    "        self.ema_anneal_end_step = self.cfg.model.ema_anneal_end_step\n",
    "\n",
    "    def _build_regression_head(self):\n",
    "        \"\"\"\n",
    "        Construct the regression head consisting of linear and activation layers.\n",
    "\n",
    "        Each modality might have its own regression block.\n",
    "\n",
    "        Returns:\n",
    "            A nn.Module layer or block of layers\n",
    "        \"\"\"\n",
    "        if self.modality == 'text':\n",
    "            return nn.Sequential(nn.Linear(self.embed_dim, self.embed_dim * 2),\n",
    "                                 nn.GELU(),\n",
    "                                 nn.Linear(self.embed_dim * 2, self.embed_dim))\n",
    "\n",
    "        if self.modality in ['audio', 'vision']:\n",
    "            return nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def ema_step(self):\n",
    "        \"\"\"\n",
    "        One EMA step for the offline model until the ending decay value is reached\n",
    "        \"\"\"\n",
    "        if self.ema_decay != self.ema_end_decay:\n",
    "            if self.ema.num_updates >= self.ema_anneal_end_step:\n",
    "                decay = self.ema_end_decay\n",
    "            else:\n",
    "                decay = self.ema.get_annealed_rate(\n",
    "                    self.ema_decay,\n",
    "                    self.ema_end_decay,\n",
    "                    self.ema.num_updates,\n",
    "                    self.ema_anneal_end_step,\n",
    "                )\n",
    "            self.ema.decay = decay\n",
    "        if self.ema.decay < 1:\n",
    "            self.ema.step(self.encoder)\n",
    "\n",
    "    def forward(self, src, trg=None, mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Data2Vec forward method.\n",
    "\n",
    "        Args:\n",
    "            src: src tokens (masked inputs for training)\n",
    "            trg: trg tokens (unmasked inputs for training but left as `None` otherwise)\n",
    "            mask: bool masked indices, Note: if a modality requires the inputs to be masked before forward this param\n",
    "            has no effect. (see the Encoder for each modality to see if it uses mask or not)\n",
    "\n",
    "        Returns:\n",
    "            Either encoder outputs or a tuple of encoder + EMA outputs\n",
    "\n",
    "        \"\"\"\n",
    "        # model forward in online mode (student)\n",
    "        x = self.encoder(src, mask, **kwargs)['encoder_out']  # fetch the last layer outputs\n",
    "        if trg is None:\n",
    "            return x\n",
    "\n",
    "        # model forward in offline mode (teacher)\n",
    "        with torch.no_grad():\n",
    "            self.ema.model.eval()\n",
    "            y = self.ema.model(trg, ~mask, **kwargs)['encoder_states']  # fetch the last transformer layers outputs\n",
    "            y = y[-self.cfg.model.average_top_k_layers:]  # take the last k transformer layers\n",
    "\n",
    "            # Follow the same layer normalization procedure for text and vision\n",
    "            if self.modality in ['vision', 'text']:\n",
    "                y = [F.layer_norm(tl.float(), tl.shape[-1:]) for tl in y]\n",
    "                y = sum(y) / len(y)\n",
    "                if self.cfg.model.normalize_targets:\n",
    "                    y = F.layer_norm(y.float(), y.shape[-1:])\n",
    "\n",
    "            # Use instance normalization for audio\n",
    "            elif self.modality == 'audio':\n",
    "                y = [F.instance_norm(tl.float()) for tl in y]\n",
    "                y = sum(y) / len(y)\n",
    "                if self.cfg.model.normalize_targets:\n",
    "                    y = F.instance_norm(y.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        x = x[mask]\n",
    "        y = y[mask]\n",
    "\n",
    "        x = self.regression_head(x)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3076e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiText(Dataset):\n",
    "    \"\"\"\n",
    "    A Dataset instance for WikiText dataset loaded from HuggingFace datasets.\n",
    "\n",
    "    Args:\n",
    "        cfg (DictConfig): config object\n",
    "        split: Split to load ['train', 'test']\n",
    "        tokenizer: A HuggingFace Tokenizer model like BPE\n",
    "        **kwargs: extra args which are set as dataset properties\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, split, tokenizer, **kwargs):\n",
    "        super(WikiText, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.path = cfg.dataset.name\n",
    "        self.mlm_probability = cfg.dataset.mlm_probability\n",
    "        raw_data = load_dataset('wikitext', self.path)[split]\n",
    "        self.data = self.clean_dataset(raw_data) if self.cfg.dataset.clean_dataset else raw_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def clean_dataset(self, data):\n",
    "        \"\"\"\n",
    "        Cleanup dataset by removing invalid sized samples, etc.\n",
    "        \"\"\"\n",
    "        print('Cleaning dataset ...')\n",
    "        min_seq_len, max_seq_len = self.cfg.dataset.valid_seq_lenghts\n",
    "        texts = []\n",
    "        with tqdm(data, desc='Removing invalid sized inputs: ') as tbar:\n",
    "            for i, x in enumerate(tbar):\n",
    "                if len(x['text']) in range(min_seq_len, max_seq_len + 1):\n",
    "                    texts.append(x)\n",
    "        return texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Only return tokens from raw text with no additions e.g, padding, bos/eos, etc.\n",
    "        Args:\n",
    "            index: sample index to pick from dataset\n",
    "\n",
    "        Returns:\n",
    "            tokenized outputs\n",
    "        \"\"\"\n",
    "        raw_text = self.data[index]['text']\n",
    "        tokens = self.tokenizer(raw_text, return_attention_mask=False)\n",
    "        return tokens\n",
    "\n",
    "    def _mask_tokens(self, inputs, special_tokens_mask=None):\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Ported\n",
    "         from `transformers.data.DataCollatorForLanguageModeling.torch_mask_tokens()`\n",
    "        Args:\n",
    "            inputs: batch of input tokens\n",
    "            special_tokens_mask:\n",
    "\n",
    "        Returns:\n",
    "            a dict batch of masked and padded inputs/labels\n",
    "\n",
    "        \"\"\"\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in\n",
    "                labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = self.tokenizer.pad_token_id\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels, masked_indices\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Collate the batch of data using BERT masking strategy. carefully ported from\n",
    "         transformers.data.DataCollatorForLanguageModeling\n",
    "        Args:\n",
    "            batch: batch of data\n",
    "\n",
    "        Returns:\n",
    "            same batch of data masked and padded\n",
    "        \"\"\"\n",
    "        batch = self.tokenizer.pad(batch, return_tensors=\"pt\")\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        src, trg, masked_indices = self._mask_tokens(\n",
    "            batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "        )\n",
    "        return src, trg, masked_indices\n",
    "\n",
    "\n",
    "#         batch = self.tokenizer.pad(batch, return_tensors=\"pt\")\n",
    "#         # If special token mask has been preprocessed, pop it from the dict.\n",
    "#         special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "#         src, trg, masked_indices = self._mask_tokens(\n",
    "#             batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "#         )\n",
    "#         return {\n",
    "#         'input_ids': src,\n",
    "#         'labels': trg,\n",
    "#         'masked_indices': masked_indices\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89aaf246",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder model using HuggingFace for NLP\n",
    "\n",
    "    To load your desired model specify model checkpoint under cfg.model.encoder_checkpoint\n",
    "\n",
    "    Args:\n",
    "        cfg: An omegaconf.DictConf instance containing all the configurations.\n",
    "        **kwargs: extra args which are set as model properties\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, **kwargs):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        checkpoint = cfg.model.encoder_checkpoint\n",
    "        model_config = AutoConfig.from_pretrained(checkpoint)\n",
    "        self.encoder = AutoModel.from_config(model_config)\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def forward(self, inputs, mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward inputs through the encoder and extract transformer/attention layers outputs\n",
    "\n",
    "        Args:\n",
    "            inputs: source tokens\n",
    "            mask: bool masked indices\n",
    "            kwargs: keyword args specific to the encoder's forward method\n",
    "\n",
    "        Returns:\n",
    "            A dictionary of the encoder outputs including transformer layers outputs and attentions outputs\n",
    "\n",
    "        \"\"\"\n",
    "        # Note: inputs are already masked for MLM so mask is not used\n",
    "        outputs = self.encoder(inputs, output_hidden_states=True, output_attentions=True, **kwargs)\n",
    "        encoder_states = outputs['hidden_states'][:-1]  # encoder layers outputs separately\n",
    "        encoder_out = outputs['hidden_states'][-1]      # last encoder output (accumulated)\n",
    "        attentions = outputs['attentions']\n",
    "        return {\n",
    "            'encoder_states': encoder_states,\n",
    "            'encoder_out': encoder_out,\n",
    "            'attentions': attentions\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0344f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def maybe_save_checkpoint(model, optimizer, path, epoch_num, save_freq):\n",
    "    \"\"\"\n",
    "    Save a checkpoint specific to Data2Vec\n",
    "    Args:\n",
    "        model: a nn.Module instance\n",
    "        optimizer\n",
    "        path: path to save checkpoint to\n",
    "        epoch_num: current epoch number\n",
    "        save_freq: save frequency based on epoch number\n",
    "\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    path = os.path.join(path, f'{epoch_num}.pt')\n",
    "    if epoch_num % save_freq == 0:\n",
    "        checkpoint = {'data2vec': model.state_dict(),\n",
    "                      'encoder': model.encoder.encoder.state_dict(),\n",
    "                      'optimizer': optimizer.state_dict()}\n",
    "        torch.save(checkpoint, path)\n",
    "        print(f'Saved checkpoint to `{path}`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9f1bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train Data2Vec for text. The encoder is loaded from huggingface specified in the config file.\n",
    "\"\"\"\n",
    "\n",
    "class TextTrainer:\n",
    "    \"\"\"\n",
    "    A Trainer class to train NLP model on Data2Vec.\n",
    "\n",
    "    Args:\n",
    "        cfg (DictConfig): the config object containing all properties\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        self.cfg = cfg\n",
    "        self.num_epochs = self.cfg.train.num_epochs\n",
    "        self.device = self.cfg.device\n",
    "        self.ckpt_dir = cfg.train.checkpoints_dir\n",
    "        self.save_ckpt_freq = cfg.train.save_ckpt_freq\n",
    "        # Model, Optim, Criterion\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(cfg.model.encoder_checkpoint)\n",
    "        self.encoder = Encoder(cfg=cfg)\n",
    "        self.model = Data2Vec(encoder=self.encoder, cfg=cfg)\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), cfg.optimizer.lr)\n",
    "        self.criterion = nn.SmoothL1Loss(reduction='none', beta=cfg.criterion.loss_beta)\n",
    "        self.criterion.to(self.device)\n",
    "        # Datasets & Data Loaders\n",
    "        self.train_dataset = WikiText(cfg, 'train', self.tokenizer)\n",
    "        self.test_dataset = WikiText(cfg, 'test', self.tokenizer)\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=cfg.train.batch_size,\n",
    "                                       collate_fn=self.train_dataset.collate_fn)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=cfg.train.eval_batch_size,\n",
    "                                      collate_fn=self.test_dataset.collate_fn)\n",
    "        # Tensorboard\n",
    "        self.tensorboard = SummaryWriter(log_dir=self.cfg.train.log_dir)\n",
    "\n",
    "        # Trackers\n",
    "        self.loss_tracker = AverageMeter('loss')\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        \"\"\"\n",
    "        Train one batch of data and return loss.\n",
    "\n",
    "        Args:\n",
    "            batch: A batch of data, inputs, labels and mask with shape [batch_size, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Loss value\n",
    "        \"\"\"\n",
    "        src, trg, mask = batch\n",
    "        src, trg, mask = src.to(self.device), trg.to(self.device), mask.to(self.device)\n",
    "\n",
    "        x, y = self.model(src, trg, mask)\n",
    "        loss = self.criterion(x.float(), y.float()).sum(dim=-1).sum().div(x.size(0))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        \"\"\"\n",
    "        Test a model on one batch of data and return loss.\n",
    "\n",
    "        Args:\n",
    "            batch: A batch of data, inputs, labels and mask with shape [batch_size, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Loss value\n",
    "        \"\"\"\n",
    "#         src = batch['input_ids'].to(self.device)\n",
    "#         trg = batch['labels'].to(self.device)\n",
    "#         mask = batch['masked_indices'].to(self.device)\n",
    "        src = batch[0].to(self.device)\n",
    "        trg = batch[1].to(self.device)\n",
    "        mask = batch[2].to(self.device)\n",
    "        x, y = self.model(src, trg, mask=mask)\n",
    "        loss = self.criterion(x, y)\n",
    "\n",
    "        return loss.item()\n",
    "#         input_ids = batch['input_ids'].to(self.device)\n",
    "#         labels = batch['labels'].to(self.device)\n",
    "#         attention_mask = batch.get('attention_mask', None)  # Add this if your data includes attention masks\n",
    "\n",
    "#         # Adjusting the call to match expected parameters\n",
    "#         outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "#         logits = outputs.logits\n",
    "\n",
    "#         loss = self.criterion(logits, labels)\n",
    "#         return loss.item()\n",
    "\n",
    "    def train_epoch(self, epoch_num):\n",
    "        \"\"\"\n",
    "        Train the model for one epoch and verbose using the progress bar.\n",
    "\n",
    "        Args:\n",
    "            epoch_num: number of the current epoch\n",
    "\n",
    "        Returns:\n",
    "            The average loss through the whole epoch\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.loss_tracker.reset()\n",
    "        with tqdm(self.train_loader, unit=\"batch\", desc=f'Epoch: {epoch_num}/{self.num_epochs} ',\n",
    "                  bar_format='{desc:<16}{percentage:3.0f}%|{bar:70}{r_bar}', ascii=\" #\") as iterator:\n",
    "            for batch in iterator:\n",
    "                loss = self.train_step(batch)\n",
    "                self.model.ema_step()\n",
    "                self.loss_tracker.update(loss)\n",
    "                avg_loss = self.loss_tracker.avg\n",
    "                iterator.set_postfix(loss=avg_loss)\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the test set\n",
    "\n",
    "        Returns:\n",
    "            The average loss through the whole test dataset\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.loss_tracker.reset()\n",
    "        with tqdm(self.test_loader, unit=\"batch\", desc=f'Evaluating... ',\n",
    "                  bar_format='{desc:<16}{percentage:3.0f}%|{bar:70}{r_bar}', ascii=\" #\") as iterator:\n",
    "            with torch.no_grad():\n",
    "                for batch in iterator:\n",
    "                    loss = self.test_step(batch)\n",
    "                    self.loss_tracker.update(loss)\n",
    "                    avg_loss = self.loss_tracker.avg\n",
    "                    iterator.set_postfix(loss=avg_loss)\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "#     def train(self):\n",
    "#         \"\"\"\n",
    "#         Train and evaluate the model on the datasets and save checkpoints and write summaries to TensorBoard.\n",
    "\n",
    "#         \"\"\"\n",
    "#         for epoch in range(1, self.num_epochs + 1):\n",
    "#             print()\n",
    "#             train_loss = self.train_epoch(epoch)\n",
    "#             val_loss = self.evaluate()\n",
    "\n",
    "#             # tensorboard\n",
    "#             self.tensorboard.add_scalar('train_loss', train_loss, epoch)\n",
    "#             self.tensorboard.add_scalar('val_loss', val_loss, epoch)\n",
    "\n",
    "#             # save checkpoint\n",
    "#             maybe_save_checkpoint(self.model, self.optimizer, self.ckpt_dir, epoch, self.save_ckpt_freq)\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Load a model from Hugging Face and evaluate on the test dataset.\n",
    "        \"\"\"\n",
    "        # Load a model compatible with your configuration from Hugging Face\n",
    "        model_checkpoint = 'bert-base-uncased'  # Make sure to use a model that fits your task\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)  # Adjust num_labels based on your task\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Evaluate the model\n",
    "        print(\"Starting evaluation using Hugging Face model...\")\n",
    "        val_loss = self.evaluate()\n",
    "        print(f\"Evaluation Loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ee63e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "cfg = OmegaConf.load('roberta-pretraining.yaml')  # Adjust the path if necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7547db90",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Run the training function\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTextTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m, in \u001b[0;36mTextTrainer.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(cfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencoder_checkpoint)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m Encoder(cfg\u001b[38;5;241m=\u001b[39mcfg)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mData2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), cfg\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mlr)\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mData2Vec.__init__\u001b[0;34m(self, encoder, cfg, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m cfg\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mema \u001b[38;5;241m=\u001b[39m \u001b[43mEMA\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# EMA acts as the teacher\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregression_head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_regression_head()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m cfg\n",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m, in \u001b[0;36mEMA.__init__\u001b[0;34m(self, model, cfg, skip_keys)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m cfg\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys \u001b[38;5;241m=\u001b[39m skip_keys \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecay \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mema_decay\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    trainer = TextTrainer(cfg)\n",
    "    trainer.train()\n",
    "\n",
    "# Run the training function\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "    trainer = TextTrainer(cfg)\n",
    "    return trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation loss:\", evaluate_model())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
