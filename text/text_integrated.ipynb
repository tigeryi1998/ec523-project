{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7940b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer,AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9f1bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTrainer:\n",
    "    \"\"\"\n",
    "    A Trainer class to train and evaluate NLP models using the Data2Vec approach.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The Data2Vec model to be trained.\n",
    "        tokenizer: Tokenizer for preparing input data.\n",
    "        train_dataset (Dataset): Dataset for training.\n",
    "        test_dataset (Dataset): Dataset for evaluation.\n",
    "        config (DictConfig): Configuration object containing training parameters and device information.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        self.config = cfg\n",
    "        self.device = cfg.device\n",
    "\n",
    "        self.encoder = Encoder(cfg=cfg)\n",
    "        self.model = Data2Vec(encoder=self.encoder, cfg=cfg)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(cfg.model.encoder_checkpoint)\n",
    "\n",
    "        self.train_dataset = WikiText(cfg, 'train', self.tokenizer)\n",
    "        self.test_dataset = WikiText(cfg, 'test', self.tokenizer)\n",
    "      \n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=cfg.train.batch_size, shuffle=True,\n",
    "                                       collate_fn=self.train_dataset.collate_fn)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=cfg.train.eval_batch_size, shuffle=False,\n",
    "                                      collate_fn=self.test_dataset.collate_fn)\n",
    "        \n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), cfg.optimizer.lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()  # Adjust according to your loss function needs\n",
    "\n",
    "    def run_epoch(self, mode='train'):\n",
    "        \"\"\"\n",
    "        Run one epoch of training or evaluation.\n",
    "\n",
    "        Args:\n",
    "            mode (str): Specifies the mode 'train' or 'eval'.\n",
    "\n",
    "        Returns:\n",
    "            Average loss of the epoch.\n",
    "        \"\"\"\n",
    "        if mode == 'train':\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(self.train_loader if mode == 'train' else self.test_loader, desc=f\"{mode.capitalize()} Epoch\"):\n",
    "            inputs, labels = batch['input_ids'].to(self.device), batch['labels'].to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "            if mode == 'train':\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(self.train_loader.dataset if mode == 'train' else self.test_loader.dataset)\n",
    "        print(f\"{mode.capitalize()} loss: {avg_loss:.4f}\")\n",
    "        return avg_loss\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        \"\"\"\n",
    "        Train the model for a given number of epochs, alternating between training and evaluation.\n",
    "\n",
    "        Args:\n",
    "            num_epochs (int): Number of epochs to train the model.\n",
    "        \"\"\"\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            self.run_epoch('train')\n",
    "            with torch.no_grad():\n",
    "                self.run_epoch('eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ee63e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "cfg = OmegaConf.load('roberta-pretraining.yaml')  # Adjust the path if necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547db90",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TextTrainer(cfg)\n",
    "num_epochs = cfg.train.num_epochs  # Number of epochs to train\n",
    "trainer.train(num_epochs)\n",
    "evaluation_loss = trainer.run_epoch(mode='eval')\n",
    "print(f\"Final Evaluation Loss: {evaluation_loss}\")\n",
    "trained_model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample text data\n",
    "examples = [\n",
    "    \"A totally engrossing thriller.\",\n",
    "    \"Unfortunately, the story is not as strong as the direction or the atmosphere.\",\n",
    "    \"This is the best movie I have ever seen.\"\n",
    "]\n",
    "\n",
    "# Load tokenizer and model from a checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model.encoder_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(trained_model)\n",
    "model.eval()\n",
    "\n",
    "# Preprocess the examples\n",
    "inputs = tokenizer(examples, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Run the model on the examples\n",
    "with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Process the results\n",
    "predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "# Map predictions to labels (for SST-2: 0 = negative, 1 = positive)\n",
    "labels = [\"negative\", \"positive\"]\n",
    "predicted_labels = [labels[p] for p in predictions]\n",
    "\n",
    "# Show results\n",
    "for text, label in zip(examples, predicted_labels):\n",
    "    print(f\"Text: {text}\\nPredicted sentiment: {label}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
