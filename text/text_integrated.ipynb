{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7940b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import copy \n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb6332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    \"\"\"\n",
    "    Exponential Moving Average (EMA) of model weights.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model for which to maintain the EMA.\n",
    "        decay (float): The decay rate for EMA (between 0 and 1).\n",
    "        device (torch.device): The device on which the EMA model will reside.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay, device=None):\n",
    "        self.decay = decay\n",
    "        self.device = device or next(model.parameters()).device\n",
    "        self.model = copy.deepcopy(model).eval()\n",
    "        self.model.to(self.device)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "    def update(self, model):\n",
    "        \"\"\"\n",
    "        Update the EMA parameters based on the current model parameters.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The current training model to update from.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            for ema_param, model_param in zip(self.model.parameters(), model.parameters()):\n",
    "                if model_param.requires_grad:\n",
    "                    ema_param.data.mul_(self.decay).add_(model_param.data, alpha=1 - self.decay)\n",
    "\n",
    "    def apply_shadow(self, model):\n",
    "        \"\"\"\n",
    "        Copy the EMA parameters to the training model.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The model to which EMA parameters will be copied.\n",
    "        \"\"\"\n",
    "        model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def __call__(self, model):\n",
    "        \"\"\"\n",
    "        Update method callable directly with the model.\n",
    "        \"\"\"\n",
    "        self.update(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f76e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data2VecText(nn.Module):\n",
    "    \"\"\"\n",
    "    Data2Vec model for text modality.\n",
    "\n",
    "    Args:\n",
    "        encoder (nn.Module): The encoder module (e.g., a Transformer-based model).\n",
    "        cfg (omegaconf.DictConfig): Configuration containing model properties.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, cfg):\n",
    "        super(Data2VecText, self).__init__()\n",
    "        self.embed_dim = cfg.model.embed_dim\n",
    "        self.encoder = encoder\n",
    "        self.cfg = cfg\n",
    "        self.ema = EMA(self.encoder, cfg.model.ema_decay, cfg.device)\n",
    "        self.regression_head = self._build_regression_head()\n",
    "\n",
    "    def _build_regression_head(self):\n",
    "        \"\"\"\n",
    "        Constructs the regression head for the model.\n",
    "\n",
    "        Returns:\n",
    "            nn.Module: A sequential model consisting of linear and activation layers.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, self.embed_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.embed_dim * 2, self.embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, src, trg=None, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through Data2Vec text model.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Source tokens (masked inputs for training).\n",
    "            trg (Tensor, optional): Target tokens (unmasked inputs for training, `None` otherwise).\n",
    "            mask (Tensor, optional): Boolean masked indices (not directly used here but can be useful for extensions).\n",
    "\n",
    "        Returns:\n",
    "            Tensor or tuple: Output from the encoder or a tuple of encoder and EMA outputs.\n",
    "        \"\"\"\n",
    "        # Model forward in online mode (student)\n",
    "        x = self.encoder(src)['encoder_out']  # Fetch the last layer outputs\n",
    "\n",
    "        if trg is None:\n",
    "            return x\n",
    "\n",
    "        # Model forward in offline mode (teacher)\n",
    "        with torch.no_grad():\n",
    "            self.ema.model.eval()\n",
    "            y = self.ema.model(trg)['encoder_states']  # Fetch the last transformer layers outputs\n",
    "\n",
    "            # Layer normalization for text\n",
    "            y = [F.layer_norm(tl.float(), tl.shape[-1:]) for tl in y]\n",
    "            y = sum(y) / len(y)\n",
    "            if self.cfg.model.normalize_targets:\n",
    "                y = F.layer_norm(y.float(), y.shape[-1:])\n",
    "\n",
    "        x = x[mask]\n",
    "        y = y[mask]\n",
    "\n",
    "        x = self.regression_head(x)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3076e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiText(Dataset):\n",
    "    \"\"\"\n",
    "    A Dataset instance for WikiText dataset loaded from HuggingFace datasets.\n",
    "\n",
    "    Args:\n",
    "        cfg (DictConfig): config object\n",
    "        split: Split to load ['train', 'test']\n",
    "        tokenizer: A HuggingFace Tokenizer model like BPE\n",
    "        **kwargs: extra args which are set as dataset properties\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, split, tokenizer, **kwargs):\n",
    "        super(WikiText, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.path = cfg.dataset.name\n",
    "        self.mlm_probability = cfg.dataset.mlm_probability\n",
    "        raw_data = load_dataset('wikitext', self.path)[split]\n",
    "        self.data = self.clean_dataset(raw_data) if self.cfg.dataset.clean_dataset else raw_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def clean_dataset(self, data):\n",
    "        \"\"\"\n",
    "        Cleanup dataset by removing invalid sized samples, etc.\n",
    "        \"\"\"\n",
    "        print('Cleaning dataset ...')\n",
    "        min_seq_len, max_seq_len = self.cfg.dataset.valid_seq_lenghts\n",
    "        texts = []\n",
    "        with tqdm(data, desc='Removing invalid sized inputs: ') as tbar:\n",
    "            for i, x in enumerate(tbar):\n",
    "                if len(x['text']) in range(min_seq_len, max_seq_len + 1):\n",
    "                    texts.append(x)\n",
    "        return texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Only return tokens from raw text with no additions e.g, padding, bos/eos, etc.\n",
    "        Args:\n",
    "            index: sample index to pick from dataset\n",
    "\n",
    "        Returns:\n",
    "            tokenized outputs\n",
    "        \"\"\"\n",
    "        raw_text = self.data[index]['text']\n",
    "        tokens = self.tokenizer(raw_text, return_attention_mask=False)\n",
    "        return tokens\n",
    "\n",
    "    def _mask_tokens(self, inputs, special_tokens_mask=None):\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Ported\n",
    "         from `transformers.data.DataCollatorForLanguageModeling.torch_mask_tokens()`\n",
    "        Args:\n",
    "            inputs: batch of input tokens\n",
    "            special_tokens_mask:\n",
    "\n",
    "        Returns:\n",
    "            a dict batch of masked and padded inputs/labels\n",
    "\n",
    "        \"\"\"\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in\n",
    "                labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = self.tokenizer.pad_token_id\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels, masked_indices\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Collate the batch of data using BERT masking strategy. carefully ported from\n",
    "         transformers.data.DataCollatorForLanguageModeling\n",
    "        Args:\n",
    "            batch: batch of data\n",
    "\n",
    "        Returns:\n",
    "            same batch of data masked and padded\n",
    "        \"\"\"\n",
    "        batch = self.tokenizer.pad(batch, return_tensors=\"pt\")\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        src, trg, masked_indices = self._mask_tokens(\n",
    "            batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "        )\n",
    "        return src, trg, masked_indices\n",
    "\n",
    "\n",
    "#         batch = self.tokenizer.pad(batch, return_tensors=\"pt\")\n",
    "#         # If special token mask has been preprocessed, pop it from the dict.\n",
    "#         special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "#         src, trg, masked_indices = self._mask_tokens(\n",
    "#             batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "#         )\n",
    "#         return {\n",
    "#         'input_ids': src,\n",
    "#         'labels': trg,\n",
    "#         'masked_indices': masked_indices\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89aaf246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder model using HuggingFace for NLP\n",
    "\n",
    "    To load your desired model specify model checkpoint under cfg.model.encoder_checkpoint\n",
    "\n",
    "    Args:\n",
    "        cfg: An omegaconf.DictConf instance containing all the configurations.\n",
    "        **kwargs: extra args which are set as model properties\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, **kwargs):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        checkpoint = cfg.model.encoder_checkpoint\n",
    "        model_config = AutoConfig.from_pretrained(checkpoint)\n",
    "        self.encoder = AutoModel.from_config(model_config)\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def forward(self, inputs, mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward inputs through the encoder and extract transformer/attention layers outputs\n",
    "\n",
    "        Args:\n",
    "            inputs: source tokens\n",
    "            mask: bool masked indices\n",
    "            kwargs: keyword args specific to the encoder's forward method\n",
    "\n",
    "        Returns:\n",
    "            A dictionary of the encoder outputs including transformer layers outputs and attentions outputs\n",
    "\n",
    "        \"\"\"\n",
    "        # Note: inputs are already masked for MLM so mask is not used\n",
    "        outputs = self.encoder(inputs, output_hidden_states=True, output_attentions=True, **kwargs)\n",
    "        encoder_states = outputs['hidden_states'][:-1]  # encoder layers outputs separately\n",
    "        encoder_out = outputs['hidden_states'][-1]      # last encoder output (accumulated)\n",
    "        attentions = outputs['attentions']\n",
    "        return {\n",
    "            'encoder_states': encoder_states,\n",
    "            'encoder_out': encoder_out,\n",
    "            'attentions': attentions\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0344f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def maybe_save_checkpoint(model, optimizer, path, epoch_num, save_freq):\n",
    "    \"\"\"\n",
    "    Save a checkpoint specific to Data2Vec\n",
    "    Args:\n",
    "        model: a nn.Module instance\n",
    "        optimizer\n",
    "        path: path to save checkpoint to\n",
    "        epoch_num: current epoch number\n",
    "        save_freq: save frequency based on epoch number\n",
    "\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    path = os.path.join(path, f'{epoch_num}.pt')\n",
    "    if epoch_num % save_freq == 0:\n",
    "        checkpoint = {'data2vec': model.state_dict(),\n",
    "                      'encoder': model.encoder.encoder.state_dict(),\n",
    "                      'optimizer': optimizer.state_dict()}\n",
    "        torch.save(checkpoint, path)\n",
    "        print(f'Saved checkpoint to `{path}`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9f1bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTrainer:\n",
    "    \"\"\"\n",
    "    Trainer class for training and evaluating the Data2VecText model on text data.\n",
    "\n",
    "    Args:\n",
    "        cfg (omegaconf.DictConfig): Configuration object containing all necessary parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.device = torch.device(cfg.device if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Initialize tokenizer, model, optimizer, and data loaders\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(cfg.model.encoder_checkpoint)\n",
    "        self.model = Data2VecText(Encoder(self.cfg), self.cfg).to(self.device)\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=cfg.optimizer.lr)\n",
    "        self.criterion = MSELoss()  # Using MSELoss for simplicity, can be changed as needed.\n",
    "\n",
    "        self.train_dataset = WikiText(cfg, 'train', self.tokenizer)\n",
    "        self.test_dataset = WikiText(cfg, 'test', self.tokenizer)\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=cfg.train.batch_size,\n",
    "                                       collate_fn=self.train_dataset.collate_fn, shuffle=True)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=cfg.train.eval_batch_size,\n",
    "                                      collate_fn=self.test_dataset.collate_fn)\n",
    "\n",
    "    def train_step(self, src, trg, mask):\n",
    "        \"\"\"\n",
    "        Executes a single training step.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Input batch of source tokens.\n",
    "            trg (Tensor): Input batch of target tokens.\n",
    "            mask (Tensor): Mask indicating the active parts of the input.\n",
    "\n",
    "        Returns:\n",
    "            float: The loss value for the step.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        x, y = self.model(src, trg, mask)\n",
    "        loss = self.criterion(x, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \"\"\"\n",
    "        Runs one epoch of training.\n",
    "\n",
    "        Returns:\n",
    "            float: Average loss for this epoch.\n",
    "        \"\"\"\n",
    "        total_loss = 0\n",
    "        for src, trg, mask in tqdm(self.train_loader, desc=\"Training\", leave=False):\n",
    "            src, trg, mask = src.to(self.device), trg.to(self.device), mask.to(self.device)\n",
    "            loss = self.train_step(src, trg, mask)\n",
    "            total_loss += loss\n",
    "        return total_loss / len(self.train_loader)\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the test dataset.\n",
    "\n",
    "        Returns:\n",
    "            float: Average loss on the test dataset.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for src, trg, mask in tqdm(self.test_loader, desc=\"Evaluating\", leave=False):\n",
    "                src, trg, mask = src.to(self.device), trg.to(self.device), mask.to(self.device)\n",
    "                x, y = self.model(src, trg, mask)\n",
    "                loss = self.criterion(x, y)\n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(self.test_loader)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Conducts the full training process across all epochs specified in the configuration.\n",
    "        \"\"\"\n",
    "        for epoch in range(1, self.cfg.train.num_epochs + 1):\n",
    "            avg_train_loss = self.train_epoch()\n",
    "            avg_val_loss = self.evaluate()\n",
    "            print(f'Epoch {epoch}/{self.cfg.train.num_epochs}, Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ee63e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "cfg = OmegaConf.load('roberta-pretraining.yaml')  # Adjust the path if necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7547db90",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Run the training function\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTextTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m, in \u001b[0;36mTextTrainer.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(cfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencoder_checkpoint)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m Encoder(cfg\u001b[38;5;241m=\u001b[39mcfg)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mData2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), cfg\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mlr)\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mData2Vec.__init__\u001b[0;34m(self, encoder, cfg, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m cfg\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mema \u001b[38;5;241m=\u001b[39m \u001b[43mEMA\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# EMA acts as the teacher\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregression_head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_regression_head()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m cfg\n",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m, in \u001b[0;36mEMA.__init__\u001b[0;34m(self, model, cfg, skip_keys)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m cfg\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys \u001b[38;5;241m=\u001b[39m skip_keys \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecay \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mema_decay\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    trainer = TextTrainer(cfg)\n",
    "    trainer.train()\n",
    "\n",
    "# Run the training function\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "    trainer = TextTrainer(cfg)\n",
    "    return trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation loss:\", evaluate_model())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
